{
 "cells": [
  {
   "cell_type": "code",
   "id": "649d298fd9b9bbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:03:36.285535Z",
     "start_time": "2025-08-23T13:03:36.193198Z"
    }
   },
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "from time import sleep as sl"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-23T13:03:38.284753Z",
     "start_time": "2025-08-23T13:03:38.281503Z"
    }
   },
   "source": [
    "def eh_pessoa(url):\n",
    "    html = urlopen(url)\n",
    "    site = html.read()\n",
    "    soup = BeautifulSoup(site, 'html.parser')\n",
    "    \n",
    "    infobox = soup.find_all(\"table\", class_=re.compile(\"^[iI]nfobox\"))\n",
    "    if infobox is not None:\n",
    "        atributos = []\n",
    "        for linha in infobox:\n",
    "            x = linha.find_all('td', {'scope': \"row\"})\n",
    "            for i in x:\n",
    "                atributos.append(i)\n",
    "    else:\n",
    "        print(\"Sem infobox\")\n",
    "    \n",
    "    for at in atributos:\n",
    "        # Criar if's com regex para encontrar atributos de pessoas\n",
    "        # Ex: Nome completo, Nome, Data de nascimento, Nascimento, Nacionalidade, Assinatura etc.\n",
    "        if re.search(\"[nN]ome\", at.text):\n",
    "            return True\n",
    "        \n",
    "    return False"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "d914c568e0c4eb16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:05:14.546923Z",
     "start_time": "2025-08-23T13:05:14.544060Z"
    }
   },
   "source": [
    "def find_url(url):\n",
    "    \"\"\"Produtor: pega links e coloca na fila\"\"\"\n",
    "    wikipedia_base_url = 'https://pt.wikipedia.org'\n",
    "    html = urlopen(url)\n",
    "    site = html.read()\n",
    "    soup = BeautifulSoup(site, \"html.parser\")\n",
    "    \n",
    "    todos_os_links_tags = soup.find_all('a', href=re.compile(r'^/wiki/'))\n",
    "    todos_os_hrefs = {tag['href'] for tag in todos_os_links_tags}\n",
    "\n",
    "    # Regex: começa com /wiki/ e não contém :, (), [], dígitos\n",
    "    padrao_regex = re.compile(r\"^/wiki/(?!.*[:()\\[\\]0-9.!])(.*_.*)$\")\n",
    "    \n",
    "    links_mantidos_hrefs = {\n",
    "        href for href in todos_os_hrefs if padrao_regex.match(href)\n",
    "    }\n",
    "    \n",
    "    links_to_search = [f'{wikipedia_base_url}{link}' for link in links_mantidos_hrefs]\n",
    "\n",
    "    # joga na fila\n",
    "    for link in links_to_search:\n",
    "        print(f\"[Produtor] Adicionando link: {link}\")\n",
    "        link_queue.put(link)\n",
    "        sl(0.1) \n"
   ],
   "outputs": [],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
